# Code generated by schema2code at 2025-08-03 11:47:11. DO NOT EDIT.


from typing import List, Dict, Optional, Any, Union
from datetime import datetime, date, time, timedelta
from .model_parameters import ModelParameters
from pydantic import BaseModel, Field, AnyUrl, EmailStr



"""GenerateReq represents a request to the Ollama embeddings API"""
class GenerateReq(BaseModel):
    # Model name
    model: str = Field(..., description="Model name")
    # Prompt to send to the model
    prompt: str = Field(..., description="Prompt to send to the model")
    # Suffix to append to the prompt
    suffix: Optional[str] = Field(None, description="Suffix to append to the prompt")
    # Images to include with the prompt
    images: Optional[List[str]] = Field(None, description="Images to include with the prompt")
    # The format to return a response in. Format can be json or a JSON schema
    format: Optional[Dict[str, Any]] = Field(None, description="The format to return a response in. Format can be json or a JSON schema")
    # Additional model parameters listed in the documentation for the Modelfile such as temperature
    options: Optional[ModelParameters] = Field(None, description="Additional model parameters listed in the documentation for the Modelfile such as temperature")
    # System message to (overrides what is defined in the Modelfile)
    system: Optional[str] = Field(None, description="System message to (overrides what is defined in the Modelfile)")
    # The prompt template to use (overrides what is defined in the Modelfile)
    template: Optional[str] = Field(None, description="The prompt template to use (overrides what is defined in the Modelfile)")
    # If false the response will be returned as a single response object
    stream: Optional[bool] = Field(None, description="If false the response will be returned as a single response object")
    # If true no formatting will be applied to the prompt
    raw: Optional[bool] = Field(None, description="If true no formatting will be applied to the prompt")
    # Controls how long the model will stay loaded into memory
    keep_alive: Optional[int] = Field(None, description="Controls how long the model will stay loaded into memory")
    # the context parameter returned from a previous request
    context: Optional[str] = Field(None, description="the context parameter returned from a previous request")
    # If true, the model will think before responding, useful for complex queries
    think: Optional[bool] = Field(None, description="If true, the model will think before responding, useful for complex queries")

    class Config:
        extra = "ignore"