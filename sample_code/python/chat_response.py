# Code generated by schema2code at 2025-08-26 22:17:46. DO NOT EDIT.


from typing import List, Dict, Optional, Any, Union
from datetime import datetime, date, time, timedelta
from .message import Message
from .message_content import MessageContent
from .message_content_type import MessageContentType
from .message_role import MessageRole
from pydantic import BaseModel, Field, AnyUrl, EmailStr, conint, confloat



class ChatResponse(BaseModel):
    """ChatResponse represents a response from the chat API"""
    done: bool = Field(..., description="Indicates whether the generation is complete")
    """Indicates whether the generation is complete"""
    message: Optional[Message] = Field(default=None, description="The message content and metadata returned by the model")
    """The message content and metadata returned by the model"""
    created_at: datetime = Field(..., description="Timestamp when the response was created")
    """Timestamp when the response was created"""
    model: str = Field(..., description="The name or identifier of the model used for generation")
    """The name or identifier of the model used for generation"""
    context: Optional[List[float]] = Field(default=None, description="Array of numbers representing the tokenized context")
    """Array of numbers representing the tokenized context"""
    finish_reason: Optional[str] = Field(default=None, description="Specific indicator of how or why the generation finished")
    """Specific indicator of how or why the generation finished"""
    total_duration: Optional[float] = Field(default=None, description="Total time taken for the entire generation process in milliseconds")
    """Total time taken for the entire generation process in milliseconds"""
    load_duration: Optional[float] = Field(default=None, description="Time taken to load the model in milliseconds")
    """Time taken to load the model in milliseconds"""
    prompt_eval_count: Optional[float] = Field(default=None, description="Number of tokens in the prompt that were evaluated")
    """Number of tokens in the prompt that were evaluated"""
    prompt_eval_duration: Optional[float] = Field(default=None, description="Time taken to evaluate the prompt tokens in milliseconds")
    """Time taken to evaluate the prompt tokens in milliseconds"""
    eval_count: Optional[float] = Field(default=None, description="Total number of tokens evaluated")
    """Total number of tokens evaluated"""
    eval_duration: Optional[float] = Field(default=None, description="Time taken for token evaluation in milliseconds")
    """Time taken for token evaluation in milliseconds"""

    class Config:
        extra = "ignore"